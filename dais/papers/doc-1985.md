---
layout: splash
permalink: /doc-1985/
title: "Did you hear that? Adversarial Examples Against Automatic Speech Recognition"
---

# Did you hear that? Adversarial Examples Against Automatic Speech Recognition

<table>
    <tbody>
    <tr>
        <td>Abstract</td>
        <td>Speech is a common and effective way of communication between humans, and modern consumer devices such as smartphones and home hubs are equipped with deep learning based accurate automatic speech recognition to enable natural interaction between humans and machines. Recently, researchers have demonstrated powerful attacks against machine learning models that can fool them to produce incorrect results. However, nearly all previous research in adversarial attacks has focused on image recognition and object detection models. In this short paper, we present a first of its kind demonstration of adversarial attacks against speech classification model. Our algorithm performs targeted attacks with 87% success by adding small background noise without having to know the underlying model parameter and architecture. Our attack only changes the least significant bits of a subset of audio clip samples, and the noise does not change 89% the human listener's perception of the audio clip as evaluated in our human study.</td>
    </tr>
    <tr>
        <td>Authors</td>
        <td>
            <ul>
                <li>Moustafa Alzantot (UCLA)</li>
                <li>Bharathan Balaji (UCLA)</li>
                <li>Mani Srivastava (UCLA)</li>
            </ul>
        </td>
    </tr>
    <tr>
        <td>Date</td>
        <td>Dec-2017</td>
    </tr>
    <tr>
        <td>Venue</td>
        <td>Machine Deception Workshop 2017</td>
    </tr>
        <tr>
            <td colspan="2">
                <form method="get" action="https://ibm.box.com/v/doc-1985-paper">
                    <button type="submit">download paper</button>
                </form>
            </td>
        </tr>
    </tbody>
</table>
