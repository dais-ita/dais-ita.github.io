---
layout: splash
permalink: /doc-3598/
title: "Model poisoning attacks against distributed machine learning systems"
---

# Model poisoning attacks against distributed machine learning systems

<table>
    <tbody>
    <tr>
        <td>Abstract</td>
        <td>Future military coalition operations will increasingly rely on machine learning (ML) methods to improve situ- ational awareness. The coalition context presents unique challenges for ML: the tactical environment creates signicant computing and communications limitations while also having to deal with an adversarial presence. Further, coalition operations must operate in a distributed manner, while coping with the constraints posed by the operational environment. Envisioned ML deployments in military assets must be resilient to these challenges. Here, we focus on the susceptibility of ML models to be poisoned (during training) or fooled (after training) by adversarial inputs. We review recent work on distributed adversarial ML, and present new results from our own investigations into model poisoning attacks on distributed learning systems without a central parameter aggregation node.</td>
    </tr>
    <tr>
        <td>Authors</td>
        <td>
            <ul>
                <li>Richard Tomsett (IBM UK)</li>
                <li>Kevin Chan (ARL)</li>
                <li>Supriyo Chakraborty (IBM US)</li>
            </ul>
        </td>
    </tr>
    <tr>
        <td>Date</td>
        <td>Apr-2019</td>
    </tr>
    <tr>
        <td>Venue</td>
        <td>SPIE - Defense + Commercial Sensing 2019</td>
    </tr>
        <tr>
            <td colspan="2">
                <form method="get" action="https://ibm.box.com/v/doc-3598-paper">
                    <button type="submit">download paper</button>
                </form>
            </td>
        </tr>
    </tbody>
</table>
