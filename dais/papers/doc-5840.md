---
layout: splash
permalink: /doc-5840/
title: "Adaptive Gradient Sparsification for Communication-Efficient Federated Learning"
---

# Adaptive Gradient Sparsification for Communication-Efficient Federated Learning

<table>
    <tbody>
    <tr>
        <td>Abstract</td>
        <td>In this paper, we consider federated learning (FL) with adaptive degree of sparsity and non-i.i.d. local dataset. To reduce the communication overhead, we first present a fairness- aware gradient sparsification (GS) method which ensures that different clients provide a similar amount of updates. Then, with the goal of minimizing the overall training time, we propose a novel online learning algorithm for automatically determining the degree of sparsity. Experiments with real datasets confirm the benefits of our proposed approaches.</td>
    </tr>
    <tr>
        <td>Authors</td>
        <td>
            <ul>
                <li>Pengchao Han (Imperial)</li>
                <li>Shiqiang Wang (IBM US)</li>
                <li>Kin Leung (Imperial)</li>
            </ul>
        </td>
    </tr>
    <tr>
        <td>Date</td>
        <td>Sep-2019</td>
    </tr>
    <tr>
        <td>Venue</td>
        <td>Annual Fall Meeting of the DAIS ITA, 2019</td>
    </tr>
        <tr>
            <td colspan="2">
                <form method="get" action="https://dais-ita.org/sites/default/files/3950_poster.pdf">
                    <button type="submit">download poster</button>
                </form>
            </td>
        </tr>
    </tbody>
</table>
