---
layout: splash
permalink: /doc-5567/
title: "Training Deep Spiking Neural Networks for Energy-Efficient Neuromorphic Computing"
---

# Training Deep Spiking Neural Networks for Energy-Efficient Neuromorphic Computing

<table>
    <tbody>
    <tr>
        <td>Abstract</td>
        <td>Spiking Neural Networks (SNNs), widely known as the third generation of neural networks, encode input information temporally using sparse spiking events, which can be harnessed to achieve higher computational efficiency for cognitive tasks. However, considering the rapid strides in accuracy enabled by state-of-the-art Analog Neural Networks (ANNs), SNN train- ing algorithms are much less mature, leading to accuracy gap between SNNs and ANNs. In this paper, we propose differ- ent SNN training methodologies, varying in degrees of bio- fidelity, and evaluate their efficacy on complex image recognition datasets. First, we present biologically plausible Spike Timing Dependent Plasticity (STDP) based deterministic and stochastic algorithms for unsupervised representation learn- ing in SNNs. Our analysis on the CIFAR-10 dataset indicates that STDP-based learning rules enable the convolutional layers to self-learn low-level input features using fewer training examples. However, STDP-based learning is limited in applicability to shallow SNNs (≤4 layers) while yielding consider- ably lower than state-of-the-art accuracy. In order to scale the SNNs deeper and improve the accuracy further, we propose conversion methodology to map off-the-shelf trained ANN to SNN for energy-efficient inference. We demonstrate 69.96% accuracy for VGG16-SNN on ImageNet. However, ANN-to- SNN conversion leads to high inference latency for achieving the best accuracy. In order to minimize the inference latency, we propose spike-based error backpropagation algorithm us- ing differentiable approximation for the spiking neuron. Our preliminary experiments on CIFAR-10 show that spike-based error backpropagation effectively captures temporal statistics to reduce the inference latency by up to 8× compared to converted SNNs while yielding comparable accuracy.</td>
    </tr>
    <tr>
        <td>Authors</td>
        <td>
            <ul>
                <li>Gopalakrishnan Srinivasan (Purdue)</li>
                <li>Chankyu Lee (Purdue)</li>
                <li>Abhronil Sengupta (PSU)</li>
                <li>Priyadarshini Panda (Purdue)</li>
                <li>Syed Shakib Sarwar (Purdue)</li>
                <li>Kaushik Roy (Purdue)</li>
            </ul>
        </td>
    </tr>
    <tr>
        <td>Date</td>
        <td>May-2020</td>
    </tr>
    <tr>
        <td>Venue</td>
        <td>ICASSP 2020 [<a href="https://ieeexplore.ieee.org/document/9053914">link</a>]</td>
    </tr>
    </tbody>
</table>
