---
layout: splash
permalink: /doc-6102/
title: "On the Performance Tradeoffs of Federated Learning in Resource Constrained Environments"
---

# On the Performance Tradeoffs of Federated Learning in Resource Constrained Environments

<table>
    <tbody>
    <tr>
        <td>Abstract</td>
        <td>In this paper, we study how to speed up federated learning in resource-constrained environments. Our work is motivated by scenarios where agents (e.g., soldiers from different coalitions) want to update a global model as soon as new data is collected to make better predictions. Previous work proposed techniques to speed up the training that trade speed for accuracy. In this work, we tackle the problem from another perspective and investigate how to train faster without sacrificing performance. The paperâ€™s contributions are to quantify the amount of gradient compression required to do not loose training performance in accelerated methods, and to characterize the model accuracy in terms of training time and the number of rounds. We also present preliminary experimental results that illustrate how a simple accelerated gradient compression scheme improves over standard gradient-descent. </td>
    </tr>
    <tr>
        <td>Authors</td>
        <td>
            <ul>
                <li>Victor Valls (Yale)</li>
                <li>Shiqiang Wang (IBM US)</li>
                <li>Kevin Chan (ARL)</li>
                <li>Kin Leung (Imperial)</li>
                <li>Leandros Tassiulas (Yale)</li>
            </ul>
        </td>
    </tr>
    <tr>
        <td>Date</td>
        <td>Sep-2020</td>
    </tr>
    <tr>
        <td>Venue</td>
        <td>4th Annual Fall Meeting of the DAIS ITA, 2020</td>
    </tr>
        <tr>
            <td colspan="2">
                <form method="get" action="https://ibm.box.com/v/doc-6102-paper">
                    <button type="submit">download paper</button>
                </form>
            </td>
        </tr>
    </tbody>
</table>
