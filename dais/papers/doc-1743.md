---
layout: splash
permalink: /doc-1743/
title: "Mitigating Adversarial Examples in Neural Networks"
---

# Mitigating Adversarial Examples in Neural Networks

<table>
    <tbody>
    <tr>
        <td>Abstract</td>
        <td>While deep neural networks models have exhibited astonishing results in a variety of tasks over the past few years, researchers have recently shown how these models can be easily fooled and attacked using adversarial examples. Adversarial examples originate from benign examples which are correctly classified by the machine learning model, but an attacker adds a small amount of noise, usually unnoticeable by a human, which are deliberately crafted to make the classifier mistake on classifying the example after adding this adversarial noise. In this paper, we study the characteristics of adversarial noise and present approaches for increasing the robustness of models against adversarial examples attacks.</td>
    </tr>
    <tr>
        <td>Authors</td>
        <td>
            <ul>
                <li>Moustafa Alzantot (UCLA)</li>
                <li>Supriyo Chakraborty (IBM US)</li>
                <li>Mani Srivastava (UCLA)</li>
            </ul>
        </td>
    </tr>
    <tr>
        <td>Date</td>
        <td>Sep-2017</td>
    </tr>
    <tr>
        <td>Venue</td>
        <td>1st Annual Fall Meeting of the DAIS ITA, 2017</td>
    </tr>
        <tr>
            <td colspan="2">
                <form method="get" action="https://dais-ita.org/sites/default/files/S_036-poster.pdf">
                    <button type="submit">download poster</button>
                </form>
            </td>
        </tr>
    </tbody>
</table>
